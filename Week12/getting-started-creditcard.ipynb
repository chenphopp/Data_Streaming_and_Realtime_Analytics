{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "getting-started.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Data_Streaming_and_Realtime_Analytics/blob/main/Week12/getting-started-creditcard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eR39We5xBb7"
      },
      "source": [
        "!pip install river"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h39R53t1w-l4"
      },
      "source": [
        "# Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IozvvRsOw-l5"
      },
      "source": [
        "First things first, make sure you have [installed river](installation.md).\n",
        "\n",
        "In `river`, features are represented with dictionaries, where the keys correspond to the features names. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:08.045146Z",
          "iopub.status.busy": "2021-10-30T00:26:08.044104Z",
          "iopub.status.idle": "2021-10-30T00:26:08.047134Z",
          "shell.execute_reply": "2021-10-30T00:26:08.047766Z"
        },
        "id": "_EzuazK1w-l6"
      },
      "source": [
        "import datetime as dt\n",
        "\n",
        "x = {\n",
        "    'shop': 'Ikea',\n",
        "    'city': 'Stockholm',\n",
        "    'date': dt.datetime(2020, 6, 1),\n",
        "    'sales': 42\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oyEkO5ixNNS",
        "outputId": "0dd9f92a-f89d-48f3-e68d-c02aa32af5cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'shop': 'Ikea', 'city': 'Stockholm', 'date': datetime.datetime(2020, 6, 1, 0, 0), 'sales': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD0Qg0HLw-l7"
      },
      "source": [
        "It is up to you, the user, to decide how to stream your data. `river` offers a `stream` module which has various utilities for handling streaming data, such as `stream.iter_csv`. For the sake of example, `river` also provides a `datasets` module which contains various streaming datasets. For example, the `datasets.Phishing` dataset contains records of [phishing](https://www.wikiwand.com/en/Phishing) attempts on web pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQUY9NiYyOLq",
        "outputId": "4d988c90-1179-4a16-fa50-547530f74719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "!pip install -U numpy "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 59 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:08.054170Z",
          "iopub.status.busy": "2021-10-30T00:26:08.052903Z",
          "iopub.status.idle": "2021-10-30T00:26:09.540169Z",
          "shell.execute_reply": "2021-10-30T00:26:09.541004Z"
        },
        "id": "BfRss4I_w-l7",
        "outputId": "d8847cd9-e5e4-4bae-85f6-cec7145a4e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from river import datasets\n",
        "\n",
        "dataset = datasets.Phishing()\n",
        "print(dataset)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phishing websites.\n",
            "\n",
            "This dataset contains features from web pages that are classified as phishing or not.\n",
            "\n",
            "    Name  Phishing                                                             \n",
            "    Task  Binary classification                                                \n",
            " Samples  1,250                                                                \n",
            "Features  9                                                                    \n",
            "  Sparse  False                                                                \n",
            "    Path  /usr/local/lib/python3.7/dist-packages/river/datasets/phishing.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ladiXTTK2shQ",
        "outputId": "18397518-3b86-4e18-d52d-90ff41ec4831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = datasets.CreditCard()\n",
        "print(dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Credit card frauds.\n",
            "\n",
            "The datasets contains transactions made by credit cards in September 2013 by european\n",
            "cardholders. This dataset presents transactions that occurred in two days, where we have 492\n",
            "frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class\n",
            "(frauds) account for 0.172% of all transactions.\n",
            "\n",
            "It contains only numerical input variables which are the result of a PCA transformation.\n",
            "Unfortunately, due to confidentiality issues, we cannot provide the original features and more\n",
            "background information about the data. Features V1, V2, ... V28 are the principal components\n",
            "obtained with PCA, the only features which have not been transformed with PCA are 'Time' and\n",
            "'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first\n",
            "transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be\n",
            "used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and\n",
            "it takes value 1 in case of fraud and 0 otherwise.\n",
            "\n",
            "      Name  CreditCard                                                     \n",
            "      Task  Binary classification                                          \n",
            "   Samples  284,807                                                        \n",
            "  Features  30                                                             \n",
            "    Sparse  False                                                          \n",
            "      Path  /root/river_data/CreditCard/creditcard.csv                     \n",
            "       URL  https://maxhalford.github.io/files/datasets/creditcardfraud.zip\n",
            "      Size  143.84 MB                                                      \n",
            "Downloaded  False                                                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4XyzgClw-l8"
      },
      "source": [
        "The dataset is a streaming dataset, and therefore doesn't sit in memory. Instead, we can loop over each sample with a `for` loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.544823Z",
          "iopub.status.busy": "2021-10-30T00:26:09.544163Z",
          "iopub.status.idle": "2021-10-30T00:26:09.553429Z",
          "shell.execute_reply": "2021-10-30T00:26:09.554042Z"
        },
        "id": "vy4qXZhsw-l8",
        "outputId": "17e23779-6522-42d0-e3a8-fcb921f91b7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for x, y in dataset:\n",
        "    pass\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://maxhalford.github.io/files/datasets/creditcardfraud.zip (65.95 MB)\n",
            "Uncompressing into /root/river_data/CreditCard\n",
            "{'Time': 172792.0, 'V1': -0.53341252200504, 'V2': -0.189733337002305, 'V3': 0.703337366963779, 'V4': -0.506271240328258, 'V5': -0.0125456787599659, 'V6': -0.649616685713792, 'V7': 1.57700625437629, 'V8': -0.414650407552662, 'V9': 0.486179505267237, 'V10': -0.915426648905893, 'V11': -1.04045833522361, 'V12': -0.0315130540252157, 'V13': -0.188092900791737, 'V14': -0.0843164698151014, 'V15': 0.0413334553360658, 'V16': -0.302620086427415, 'V17': -0.660376645182784, 'V18': 0.16742993371973, 'V19': -0.256116871098099, 'V20': 0.382948104875066, 'V21': 0.261057330790975, 'V22': 0.643078437820093, 'V23': 0.376777014169917, 'V24': 0.00879737940024202, 'V25': -0.473648703898825, 'V26': -0.818267121041176, 'V27': -0.00241530880001015, 'V28': 0.0136489143320671, 'Amount': 217.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.557333Z",
          "iopub.status.busy": "2021-10-30T00:26:09.556790Z",
          "iopub.status.idle": "2021-10-30T00:26:09.558855Z",
          "shell.execute_reply": "2021-10-30T00:26:09.559238Z"
        },
        "id": "E3IJfKo8w-l8",
        "outputId": "39d01c0b-abf3-4b00-ce4f-2775da506a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFz1V6iQw-l9"
      },
      "source": [
        "Typically, models learn via a `learn_one(x, y)` method, which takes as input some features and a target value. Being able to learn with a single instance gives a lot of flexibility. For instance, a model can be updated whenever a new sample arrives from a stream. To exemplify this, let's train a logistic regression on the above dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.588727Z",
          "iopub.status.busy": "2021-10-30T00:26:09.562421Z",
          "iopub.status.idle": "2021-10-30T00:26:09.590394Z",
          "shell.execute_reply": "2021-10-30T00:26:09.590849Z"
        },
        "id": "fta0RpqLw-l9"
      },
      "source": [
        "from river import linear_model\n",
        "\n",
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "for x, y in dataset:\n",
        "    model.learn_one(x, y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyU40A4Aw-l9"
      },
      "source": [
        "Predictions can be obtained by calling a model's `predict_one` method. In the case of a classifier, we can also use `predict_proba_one` to produce probability estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.594423Z",
          "iopub.status.busy": "2021-10-30T00:26:09.593879Z",
          "iopub.status.idle": "2021-10-30T00:26:09.626053Z",
          "shell.execute_reply": "2021-10-30T00:26:09.626603Z"
        },
        "id": "4pnXtfB1w-l-",
        "outputId": "2b0410ee-5551-43fa-8072-265adcfa27f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "for x, y in dataset:\n",
        "    y_pred = model.predict_proba_one(x)\n",
        "    model.learn_one(x, y)\n",
        "    \n",
        "print(y_pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{False: 1.0, True: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fq_iuh0w-l-"
      },
      "source": [
        "The `metrics` module gives access to many metrics that are commonly used in machine learning. Like the rest of `river`, these metrics can be updated with one element at a time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.636277Z",
          "iopub.status.busy": "2021-10-30T00:26:09.630086Z",
          "iopub.status.idle": "2021-10-30T00:26:09.833325Z",
          "shell.execute_reply": "2021-10-30T00:26:09.833948Z"
        },
        "id": "Ux4eQXCDw-l-",
        "outputId": "501ff44a-8d0f-41e6-80f4-bce79dfcda7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from river import metrics\n",
        "\n",
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "metric = metrics.ROCAUC()\n",
        "\n",
        "for x, y in dataset:\n",
        "    y_pred = model.predict_proba_one(x)\n",
        "    model.learn_one(x, y)\n",
        "    metric.update(y, y_pred)\n",
        "    \n",
        "metric"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ROCAUC: 0.528647"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJGTC-7Gw-l-"
      },
      "source": [
        "A common way to improve the performance of a logistic regression is to scale the data. This can be done by using a `preprocessing.StandardScaler`. In particular, we can define a pipeline to organise our model into a sequence of steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.838280Z",
          "iopub.status.busy": "2021-10-30T00:26:09.837731Z",
          "iopub.status.idle": "2021-10-30T00:26:09.841617Z",
          "shell.execute_reply": "2021-10-30T00:26:09.842063Z"
        },
        "id": "R2KGQib8w-l_",
        "outputId": "3a55ffee-c63b-45c9-b862-6dd5b04d7dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "from river import compose\n",
        "from river import preprocessing\n",
        "\n",
        "model = compose.Pipeline(\n",
        "    preprocessing.StandardScaler(),\n",
        "    linear_model.LogisticRegression()\n",
        ")\n",
        "\n",
        "model"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<html><body><div class=\"pipeline\"><details class=\"estimator\"><summary><pre class=\"estimator-name\">StandardScaler</pre></summary><code class=\"estimator-params\">\n",
              "{'counts': Counter(),\n",
              " 'means': defaultdict(&lt;class 'float'&gt;, {}),\n",
              " 'vars': defaultdict(&lt;class 'float'&gt;, {}),\n",
              " 'with_std': True}\n",
              "\n",
              "</code></details><details class=\"estimator\"><summary><pre class=\"estimator-name\">LogisticRegression</pre></summary><code class=\"estimator-params\">\n",
              "{'_weights': {},\n",
              " '_y_name': None,\n",
              " 'clip_gradient': 1000000000000.0,\n",
              " 'initializer': Zeros (),\n",
              " 'intercept': 0.0,\n",
              " 'intercept_init': 0.0,\n",
              " 'intercept_lr': Constant({'learning_rate': 0.01}),\n",
              " 'l2': 0.0,\n",
              " 'loss': Log({'weight_pos': 1.0, 'weight_neg': 1.0}),\n",
              " 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 0})}\n",
              "\n",
              "</code></details></div></body><style>\n",
              ".estimator {\n",
              "    padding: 1em;\n",
              "    border-style: solid;\n",
              "    background: white;\n",
              "}\n",
              "\n",
              ".pipeline {\n",
              "    display: flex;\n",
              "    flex-direction: column;\n",
              "    align-items: center;\n",
              "    background: linear-gradient(#000, #000) no-repeat center / 3px 100%;\n",
              "}\n",
              "\n",
              ".union {\n",
              "    display: flex;\n",
              "    flex-direction: row;\n",
              "    align-items: center;\n",
              "    justify-content: center;\n",
              "    padding: 1em;\n",
              "    border-style: solid;\n",
              "    background: white\n",
              "}\n",
              "\n",
              "/* Vertical spacing between steps */\n",
              "\n",
              ".estimator + .estimator,\n",
              ".estimator + .union,\n",
              ".union + .estimator {\n",
              "    margin-top: 2em;\n",
              "}\n",
              "\n",
              ".union > .estimator {\n",
              "    margin-top: 0;\n",
              "}\n",
              "\n",
              "/* Spacing within a union of estimators */\n",
              "\n",
              ".union >\n",
              ".estimator + .estimator,\n",
              ".pipeline + .estimator,\n",
              ".estimator + .pipeline,\n",
              ".pipeline + .pipeline {\n",
              "    margin-left: 1em;\n",
              "}\n",
              "\n",
              "/* Typography */\n",
              ".estimator-params {\n",
              "    display: block;\n",
              "    white-space: pre-wrap;\n",
              "    font-size: 120%;\n",
              "    margin-bottom: -1em;\n",
              "}\n",
              "\n",
              ".estimator > code {\n",
              "    background-color: white !important;\n",
              "}\n",
              "\n",
              ".estimator-name {\n",
              "    display: inline;\n",
              "    margin: 0;\n",
              "    font-size: 130%;\n",
              "}\n",
              "\n",
              "/* Toggle */\n",
              "\n",
              "summary {\n",
              "    display: flex;\n",
              "    align-items:center;\n",
              "    cursor: pointer;\n",
              "}\n",
              "\n",
              "summary > div {\n",
              "    width: 100%;\n",
              "}\n",
              "</style></html>"
            ],
            "text/plain": [
              "Pipeline (\n",
              "  StandardScaler (\n",
              "    with_std=True\n",
              "  ),\n",
              "  LogisticRegression (\n",
              "    optimizer=SGD (\n",
              "      lr=Constant (\n",
              "        learning_rate=0.01\n",
              "      )\n",
              "    )\n",
              "    loss=Log (\n",
              "      weight_pos=1.\n",
              "      weight_neg=1.\n",
              "    )\n",
              "    l2=0.\n",
              "    intercept_init=0.\n",
              "    intercept_lr=Constant (\n",
              "      learning_rate=0.01\n",
              "    )\n",
              "    clip_gradient=1e+12\n",
              "    initializer=Zeros ()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T00:26:09.895357Z",
          "iopub.status.busy": "2021-10-30T00:26:09.845201Z",
          "iopub.status.idle": "2021-10-30T00:26:10.087073Z",
          "shell.execute_reply": "2021-10-30T00:26:10.087528Z"
        },
        "id": "BUV5dFprw-l_",
        "outputId": "20088c66-3711-4cae-ec51-b81ca1a83924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "metric = metrics.ROCAUC()\n",
        "\n",
        "for x, y in dataset:\n",
        "    y_pred = model.predict_proba_one(x)\n",
        "    model.learn_one(x, y)\n",
        "    metric.update(y, y_pred)\n",
        "    \n",
        "metric"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ROCAUC: 0.891088"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY8j2SKhw-l_"
      },
      "source": [
        "As we can see, the model is performing much better now that the data is being scaled. Under the hood, the standard scaler maintains a running average and a running variance for each feature in the dataset. Each feature is thus scaled according to the average and the variance seen up to every given point in time.\n",
        "\n",
        "This concludes this short guide to getting started with `river`. There is a lot more to discover and understand. Head towards the [user guide](../user-guide/reading-data.md) for recipes on how to perform common machine learning tasks. You may also consult the [API reference](../api/overview.md), which is a catalogue of all the modules that `river` exposes. Finally, the [examples](../examples/batch-to-online.md) section contains comprehensive examples for various usecases."
      ]
    }
  ]
}