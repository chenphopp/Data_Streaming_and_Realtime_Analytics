{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week7-twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML7OB8jHMBGcU226Oomi5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Data_Streaming_and_Realtime_Analytics/blob/main/Week07/Week7_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX3qY84P4aHy"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://mirrors.estointernet.in/apache/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\"\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qik26EJg4RZy"
      },
      "source": [
        "# importing required libraries\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pyspark.sql.types as tp\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import Row, Column\n",
        "import sys\n",
        "from pyspark import SparkFiles"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get prediction"
      ],
      "metadata": {
        "id": "og8EfZlxPN3b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B0onOFO3mvM"
      },
      "source": [
        "# define the function to get the predicted sentiment on the data received\n",
        "def get_prediction(tweet_text):\n",
        "  try:\n",
        "    \n",
        "    # remove the blank tweets\n",
        "    #print(\"remvoe the blank tweets\")\n",
        "    tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
        "    print(\"tweet_text \", tweet_text)\n",
        "    \n",
        "    # create the dataframe with each row contains a tweet text\n",
        "    #print(\"create the dataframe\")\n",
        "    rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
        "    print(\"rowRdd \",rowRdd)\n",
        "\n",
        "    wordsDataFrame = spark.createDataFrame(rowRdd)\n",
        "    print(\"wordsDataFrame \",wordsDataFrame)\n",
        "\n",
        "\t\t# get the sentiments for each row\n",
        "    pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
        "  \n",
        "  except : \n",
        "    print('No data')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdQnqg5V8GAs"
      },
      "source": [
        "sc = SparkContext(appName=\"PySparkShell\")\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "cDzUBCDmPK-s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5QZRvUq6_jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb6fc91-e9e2-485c-f834-1d13addc8d70"
      },
      "source": [
        "#1 not good\n",
        "#0 okay\n",
        "url = \"https://raw.githubusercontent.com/lakshay-arora/PySpark/master/spark_streaming/datasets/twitter_sentiments.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "# define the schema\n",
        "my_schema = tp.StructType([\n",
        "        tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),\n",
        "        tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),\n",
        "        tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)    \n",
        "            ])\n",
        "# reading the data set\n",
        "print('\\n\\nReading the dataset...........................\\n')\n",
        "my_data = spark.read.csv(\"file://\"+SparkFiles.get(\"twitter_sentiments.csv\"), schema=my_schema, header=True)\n",
        "my_data.show(5) # like head() in pandas\n",
        "\n",
        "my_data.printSchema()\n",
        "print('\\n\\nDefining the pipeline stages.................\\n')\n",
        "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
        "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
        "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
        "\n",
        "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label') \n",
        "\n",
        "print('\\n\\nStages Defined................................\\n')\n",
        "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
        "\n",
        "print('\\n\\nFit the pipeline with the training data.......\\n')\n",
        "pipelineFit = pipeline.fit(my_data)\n",
        "\n",
        "print('\\n\\nModel Trained....Waiting for the Data!!!!!!!!\\n')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Reading the dataset...........................\n",
            "\n",
            "+---+-----+--------------------+\n",
            "| id|label|               tweet|\n",
            "+---+-----+--------------------+\n",
            "|  1|    0| @user when a fat...|\n",
            "|  2|    0|@user @user thank...|\n",
            "|  3|    0|  bihday your maj...|\n",
            "|  4|    0|#model   i love u...|\n",
            "|  5|    0| factsguide: soci...|\n",
            "+---+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- tweet: string (nullable = true)\n",
            "\n",
            "\n",
            "\n",
            "Defining the pipeline stages.................\n",
            "\n",
            "\n",
            "\n",
            "Stages Defined................................\n",
            "\n",
            "\n",
            "\n",
            "Fit the pipeline with the training data.......\n",
            "\n",
            "\n",
            "\n",
            "Model Trained....Waiting for the Data!!!!!!!!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQrt3sVqJ9Hr"
      },
      "source": [
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "kafka_topic_name = \"my-first-topic\"\n",
        "kafka_bootstrap_servers = 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BavudqOjCWW2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "c7c6b867-0193-43b7-9d7b-abc63110df9a"
      },
      "source": [
        "ssc = StreamingContext(sc, 5)\n",
        "kvs = KafkaUtils.createStream(ssc, kafka_bootstrap_servers, 'spark-streaming-consumer', {kafka_topic_name:1}) \n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\"metadata.broker.list\": kafka_bootstrap_servers})\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'largest'})\n",
        "\n",
        "lines = kvs.map(lambda x: x[1])\n",
        "\n",
        "words = lines.flatMap(lambda line : line.split(' '))\n",
        "#words.pprint()\n",
        "\n",
        "words.foreachRDD(get_prediction)\n",
        "\n",
        "ssc.start()             # Start the computation\n",
        "ssc.awaitTermination()  # Wait for the computation to terminate"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweet_text  PythonRDD[160] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[161] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[167] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[168] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[174] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[175] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[182] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[183] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[189] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[190] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[196] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[197] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[204] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[205] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[211] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[212] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[218] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[219] at RDD at PythonRDD.scala:53\n",
            "No data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5d1460808992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the computation to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePQQWVY8BHD2",
        "outputId": "925424e3-5928-4383-faee-22a7f91c59b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweet_text  PythonRDD[226] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[227] at RDD at PythonRDD.scala:53\n",
            "No data\n",
            "tweet_text  PythonRDD[233] at RDD at PythonRDD.scala:53\n",
            "rowRdd  PythonRDD[234] at RDD at PythonRDD.scala:53\n",
            "No data\n"
          ]
        }
      ]
    }
  ]
}