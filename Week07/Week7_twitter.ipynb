{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week7-twitter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7IZVE+7T8IFMz5zDy3Tjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Data_Streaming_and_Realtime_Analytics/blob/main/Week7_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX3qY84P4aHy"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\"\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qik26EJg4RZy"
      },
      "source": [
        "# importing required libraries\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pyspark.sql.types as tp\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import Row, Column\n",
        "import sys\n",
        "from pyspark import SparkFiles"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B0onOFO3mvM"
      },
      "source": [
        "# define the function to get the predicted sentiment on the data received\n",
        "def get_prediction(tweet_text):\n",
        "  try:\n",
        "    \n",
        "    # remove the blank tweets\n",
        "    tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
        "    \n",
        "    # create the dataframe with each row contains a tweet text\n",
        "    rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
        "    wordsDataFrame = spark.createDataFrame(rowRdd)\n",
        "\t\t# get the sentiments for each row\n",
        "    pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
        "  \n",
        "  except : \n",
        "    print('No data')\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5QZRvUq6_jI"
      },
      "source": [
        "sc = SparkContext(appName=\"PySparkShell\")\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/lakshay-arora/PySpark/master/spark_streaming/datasets/twitter_sentiments.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "# define the schema\n",
        "my_schema = tp.StructType([\n",
        "        tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),\n",
        "        tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),\n",
        "        tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)    \n",
        "            ])\n",
        "# reading the data set\n",
        "print('\\n\\nReading the dataset...........................\\n')\n",
        "my_data = spark.read.csv(\"file://\"+SparkFiles.get(\"twitter_sentiments.csv\"), schema=my_schema, header=True)\n",
        "my_data.show(2)\n",
        "\n",
        "my_data.printSchema()\n",
        "print('\\n\\nDefining the pipeline stages.................\\n')\n",
        "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
        "\n",
        "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
        "\n",
        "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
        "\n",
        "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label') \n",
        "\n",
        "print('\\n\\nStages Defined................................\\n')\n",
        "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
        "\n",
        "print('\\n\\nFit the pipeline with the training data.......\\n')\n",
        "pipelineFit = pipeline.fit(my_data)\n",
        "\n",
        "print('\\n\\nModel Trained....Waiting for the Data!!!!!!!!\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQrt3sVqJ9Hr"
      },
      "source": [
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "kafka_topic_name = \"my-first-topic\"\n",
        "kafka_bootstrap_servers = 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BavudqOjCWW2"
      },
      "source": [
        "ssc = StreamingContext(sc, batchDuration= 3)\n",
        "kvs = KafkaUtils.createStream(ssc, kafka_bootstrap_servers, 'spark-streaming-consumer', {kafka_topic_name:1}) \n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\"metadata.broker.list\": kafka_bootstrap_servers})\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'largest'})\n",
        "\n",
        "lines = kvs.map(lambda x: x[1])\n",
        "words = lines.flatMap(lambda line : line.split('TWEET_APP'))\n",
        "\n",
        "words.foreachRDD(get_prediction)\n",
        "\n",
        "ssc.start()             # Start the computation\n",
        "ssc.awaitTermination()  # Wait for the computation to terminate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePQQWVY8BHD2"
      },
      "source": [
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}