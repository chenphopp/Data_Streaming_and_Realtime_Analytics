{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6-spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEqtgG3oVcoMA7TpoZmIvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaratnida/Data_Streaming_and_Realtime_Analytics/blob/main/Week06/Week6_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download & setup"
      ],
      "metadata": {
        "id": "5Vth07tk-Udu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6F2kL0gBEEG"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ouU2CBRBqUd",
        "outputId": "51a06737-f567-47dc-ae16-f87042dbbbbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-27 02:54:01--  https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12002039 (11M) [application/java-archive]\n",
            "Saving to: ‘spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar’\n",
            "\n",
            "\r          spark-str   0%[                    ]       0  --.-KB/s               \rspark-streaming-kaf 100%[===================>]  11.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-27 02:54:02 (114 MB/s) - ‘spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar’ saved [12002039/12002039]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python"
      ],
      "metadata": {
        "id": "J1LWVX6A-SAt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KruJyzUTB39c"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q-ssxltCGgc"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W5QS4W7CtYO"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import random\n",
        "import pyspark\n",
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "from uuid import uuid1\n",
        "import time\n",
        "\n",
        "kafka_topic_name = \"my-first-topic\"\n",
        "kafka_bootstrap_servers = 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU6RCJlEFm5V"
      },
      "source": [
        "sc = pyspark.SparkContext()\n",
        "ssc = StreamingContext(sc,5) # Show results every 5 seconds\n",
        "\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'largest'})\n",
        "\n",
        "step0 = kvs.map(lambda x: x[1])\n",
        "#print(\"step0\") # print once\n",
        "step0.pprint()\n",
        "step1 = step0.flatMap(lambda line: line.split(' '))\n",
        "step1.pprint()\n",
        "step2 = step1.map(lambda word: (word, 1))\n",
        "step2.pprint()\n",
        "step3 = step2.reduceByKey(lambda a, b: a+b) #.filter(lambda y: y[1]>5)\n",
        "step3.pprint()\n",
        "\n",
        "ssc.start()\n",
        "# stream will run for 100 sec\n",
        "ssc.awaitTerminationOrTimeout(100)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "UuLxA4lrCxi8"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}